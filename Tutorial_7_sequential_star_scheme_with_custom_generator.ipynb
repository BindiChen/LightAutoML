{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.0 Install LightAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U lightautoml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python libraries\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Installed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports from lightautoml package\n",
    "from lightautoml.automl.base import AutoML\n",
    "from lightautoml.ml_algo.boost_lgbm import BoostLGBM\n",
    "\n",
    "from lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\n",
    "from lightautoml.pipelines.ml.base import MLPipeline\n",
    "from lightautoml.reader.base import DictToNumpySeqReader\n",
    "from lightautoml.tasks import Task\n",
    "\n",
    "# Import Feature Generator Transformer\n",
    "from lightautoml.pipelines.features.generator_pipeline import FeatureGeneratorPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.2 Example data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data filepaths\n",
    "data_dir = 'data2'\n",
    "\n",
    "filepaths = {\n",
    "    'app_train': pjoin(data_dir, 'application_train.zip'),\n",
    "    'app_test': pjoin(data_dir, 'application_test.zip'),\n",
    "    'bureau': pjoin(data_dir, 'bureau.zip'),\n",
    "    'credit_bl': pjoin(data_dir, 'credit_card_balance.zip'),\n",
    "    'install_pays': pjoin(data_dir, 'installments_payments.zip'),\n",
    "    'pc_balance': pjoin(data_dir, 'POS_CASH_balance.zip'),\n",
    "    'app_prev': pjoin(data_dir, 'previous_application.zip'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_train (5000, 122)\n",
      "app_test (5000, 121)\n",
      "bureau (49227, 17)\n",
      "credit_bl (113392, 23)\n",
      "install_pays (392693, 8)\n",
      "pc_balance (287697, 8)\n",
      "app_prev (49100, 37)\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "for df_name in filepaths.keys():\n",
    "    dataframes[df_name] = pd.read_csv(filepaths[df_name], encoding='latin1')\n",
    "    print(df_name, dataframes[df_name].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.3 Create sequential star scheme dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_params = {'bureau': {'case': 'ids',\n",
    "                       'params': {},\n",
    "                       'scheme': {'to': 'plain', 'from_id': 'SK_ID_CURR', 'to_id': 'SK_ID_CURR'},\n",
    "                       },\n",
    "             'app_prev': {'case': 'ids',\n",
    "                       'params': {},\n",
    "                       'scheme': {'to': 'plain', 'from_id': 'SK_ID_CURR', 'to_id': 'SK_ID_CURR'},\n",
    "                         },\n",
    "             'credit_bl': {'case': 'ids',\n",
    "                       'params': {},\n",
    "                       'scheme': {'to': 'plain', 'from_id': 'SK_ID_CURR', 'to_id': 'SK_ID_CURR'},\n",
    "                          },\n",
    "              'install_pays':{'case': 'ids',\n",
    "                       'params': {},\n",
    "                       'scheme': {'to': 'plain', 'from_id': 'SK_ID_CURR', 'to_id': 'SK_ID_CURR'},\n",
    "                          },\n",
    "              'pc_balance':{'case': 'ids',\n",
    "                       'params': {},\n",
    "                       'scheme': {'to': 'plain', 'from_id': 'SK_ID_CURR', 'to_id': 'SK_ID_CURR'},\n",
    "                          },                  \n",
    "                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict with second-level tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = {'bureau': dataframes['bureau'],\n",
    "       'app_prev': dataframes['app_prev'],\n",
    "       'credit_bl': dataframes['credit_bl'],\n",
    "       'install_pays': dataframes['install_pays'],\n",
    "       'pc_balance': dataframes['pc_balance']              \n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train and test data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {'plain':dataframes['app_train'] , \n",
    "           'seq': seq_data\n",
    "          }\n",
    "\n",
    "X_test = {'plain':dataframes['app_test'] , \n",
    "          'seq': seq_data\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create Task snd Sequential Reader for the star scheme data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task('binary', metric='logloss')\n",
    "roles={'target': 'TARGET'}\n",
    "reader = DictToNumpySeqReader(task=task, seq_params=seq_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Create Feature Generator Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define interesing values for feature generation in corresponding slices (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_values = {\n",
    "    'bureau': {'CREDIT_ACTIVE': ['Active', 'Closed']},\n",
    "    'app_prev': {'NAME_CONTRACT_TYPE': ['Consumer']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 122)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['app_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params of feature generator:\n",
    "- seq_params: sequence-related params.\n",
    "- max_gener_features: maximum generated features.\n",
    "- max_depth: maximum allowed depth of features.\n",
    "- agg_primitives: list of aggregation primitives.\n",
    "- trans_primitives: list of transform primitives.\n",
    "- interesting_values: categorical values if the form of {table_name: {column: [values]}} for feature generation in corresponding slices.\n",
    "- generate_interesting_values: whether generate feature in slices of unique categories or not.\n",
    "- per_top_categories: percent of most frequent categories for feature generation in corresponding slices. If number of unique values is less than 10, then the all values are be used. \n",
    "- sample_size: size of data to make generated feature selection on it.\n",
    "- n_jobs: number of processes to run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FeatureGeneratorPipeline(seq_params,\n",
    "                                     max_gener_features=500,\n",
    "                                     interesting_values = interesting_values,\n",
    "                                     generate_interesting_values = True,\n",
    "                                     per_top_categories = 25,\n",
    "                                     sample_size = None,\n",
    "                                     n_jobs = 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Create one-level ML pipeline for AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleransf = LGBSimpleFeatures()\n",
    "feats = generator.append(simpleransf)\n",
    "\n",
    "model = BoostLGBM()\n",
    "\n",
    "pipeline_lvl1 = MLPipeline([model], pre_selection=None,\n",
    "                           features_pipeline=feats, \n",
    "                           post_selection=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML(reader, [\n",
    "    [pipeline_lvl1],\n",
    "], skip_conn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Train AutoML on loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:23:11] Layer \u001b[1m1\u001b[0m train process start. Time left 9999999992.53 secs\n",
      "[20:23:11] This selector only for holdout training. fit_on_holout argument added just to be compatible\n",
      "[20:23:11] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "EntitySet scattered to 16 workers in 22 seconds\n",
      "[20:24:00] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:01] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:04] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:07] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:09] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:11] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:14] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:16] \u001b[1mLightGBM\u001b[0m fitting and predicting completed\n",
      "EntitySet scattered to 16 workers in 20 seconds\n",
      "[20:24:46] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
      "[20:24:46] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[20:24:48] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[20:24:51] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[20:24:54] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[20:24:57] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[20:24:59] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m-0.24695388874122873\u001b[0m\n",
      "[20:24:59] \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[20:24:59] Time left 9999999884.32 secs\n",
      "\n",
      "[20:24:59] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "CPU times: user 2min 46s, sys: 7.98 s, total: 2min 54s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_pred = automl.fit_predict(X_train, roles=roles, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Analyze fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EXT_SOURCE_3                                       804.856992\n",
       "EXT_SOURCE_2                                       721.087537\n",
       "EXT_SOURCE_1                                       159.317814\n",
       "DAYS_ID_PUBLISH                                    131.567411\n",
       "ft__plain_SK_ID_CURR.MEDIAN(bureau.DAYS_CREDIT)    130.531718\n",
       "                                                      ...    \n",
       "ord__FLAG_OWN_CAR                                    0.000000\n",
       "FLAG_DOCUMENT_6                                      0.000000\n",
       "FLAG_PHONE                                           0.000000\n",
       "FLAG_EMP_PHONE                                       0.000000\n",
       "FLAG_DOCUMENT_5                                      0.000000\n",
       "Length: 244, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imps = model.get_features_score()\n",
    "feature_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EXT_SOURCE_3', 'EXT_SOURCE_2', 'EXT_SOURCE_1', 'DAYS_ID_PUBLISH',\n",
       "       'ft__plain_SK_ID_CURR.MEDIAN(bureau.DAYS_CREDIT)',\n",
       "       'ft__plain_SK_ID_CURR.MEAN(bureau.DAYS_CREDIT)',\n",
       "       'ft__plain_SK_ID_CURR.MIN(install_pays.AMT_PAYMENT)',\n",
       "       'ft__plain_SK_ID_CURR.STD(install_pays.DAYS_ENTRY_PAYMENT)',\n",
       "       'DAYS_EMPLOYED',\n",
       "       'ft__plain_SK_ID_CURR.MEDIAN(bureau.DAYS_CREDIT_UPDATE)',\n",
       "       ...\n",
       "       'ord__WALLSMATERIAL_MODE', 'ord__NAME_CONTRACT_TYPE',\n",
       "       'ord__NONLIVINGAPARTMENTS_AVG', 'FLAG_WORK_PHONE', 'FLOORSMIN_MODE',\n",
       "       'FLOORSMIN_MEDI', 'FLAG_EMAIL', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       "       'FLOORSMAX_MEDI', 'FLAG_DOCUMENT_8'],\n",
       "      dtype='object', length=220)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imps.index[feature_imps > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-loss: 0.24695388874122873, roc_auc: 0.7370297876927845\n"
     ]
    }
   ],
   "source": [
    "metric_l = log_loss(X_train['plain'][roles['target']], train_pred.data[:, 0])\n",
    "metric_a = roc_auc_score(X_train['plain'][roles['target']], train_pred.data[:, 0])\n",
    "\n",
    "print(f'log-loss: {metric_l}, roc_auc: {metric_a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to 16 workers in 22 seconds\n"
     ]
    }
   ],
   "source": [
    "test_pred = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Pickle the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to 16 workers in 21 seconds\n"
     ]
    }
   ],
   "source": [
    "with open('ft_model.pickle', 'wb') as f:\n",
    "    pickle.dump(automl, f)\n",
    "    \n",
    "with open('ft_model.pickle', 'rb') as f:\n",
    "    automl = pickle.load(f)\n",
    "_pred = automl.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
